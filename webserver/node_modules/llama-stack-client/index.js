"use strict";
// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var _a;
Object.defineProperty(exports, "__esModule", { value: true });
exports.fileFromPath = exports.toFile = exports.UnprocessableEntityError = exports.PermissionDeniedError = exports.InternalServerError = exports.AuthenticationError = exports.BadRequestError = exports.RateLimitError = exports.ConflictError = exports.NotFoundError = exports.APIUserAbortError = exports.APIConnectionTimeoutError = exports.APIConnectionError = exports.APIError = exports.LlamaStackClientError = exports.LlamaStackClient = void 0;
const Errors = __importStar(require("./error.js"));
const Uploads = __importStar(require("./uploads.js"));
const Core = __importStar(require("./core.js"));
const API = __importStar(require("./resources/index.js"));
const environments = {
    production: 'http://any-hosted-llama-stack-client.com',
    sandbox: 'https://example.com',
};
/**
 * API Client for interfacing with the Llama Stack Client API.
 */
class LlamaStackClient extends Core.APIClient {
    /**
     * API Client for interfacing with the Llama Stack Client API.
     *
     * @param {Environment} [opts.environment=production] - Specifies the environment URL to use for the API.
     * @param {string} [opts.baseURL=process.env['LLAMA_STACK_CLIENT_BASE_URL'] ?? http://any-hosted-llama-stack-client.com] - Override the default base URL for the API.
     * @param {number} [opts.timeout=1 minute] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.
     * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.
     * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.
     * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.
     * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.
     * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.
     */
    constructor({ baseURL = Core.readEnv('LLAMA_STACK_CLIENT_BASE_URL'), ...opts } = {}) {
        const options = {
            ...opts,
            baseURL,
            environment: opts.environment ?? 'production',
        };
        if (baseURL && opts.environment) {
            throw new Errors.LlamaStackClientError('Ambiguous URL; The `baseURL` option (or LLAMA_STACK_CLIENT_BASE_URL env var) and the `environment` option are given. If you want to use the environment you must pass baseURL: null');
        }
        super({
            baseURL: options.baseURL || environments[options.environment || 'production'],
            timeout: options.timeout ?? 60000 /* 1 minute */,
            httpAgent: options.httpAgent,
            maxRetries: options.maxRetries,
            fetch: options.fetch,
        });
        this.telemetry = new API.Telemetry(this);
        this.agents = new API.Agents(this);
        this.datasets = new API.Datasets(this);
        this.evaluate = new API.Evaluate(this);
        this.evaluations = new API.Evaluations(this);
        this.inference = new API.Inference(this);
        this.safety = new API.Safety(this);
        this.memory = new API.Memory(this);
        this.postTraining = new API.PostTraining(this);
        this.rewardScoring = new API.RewardScoringResource(this);
        this.syntheticDataGeneration = new API.SyntheticDataGenerationResource(this);
        this.batchInference = new API.BatchInference(this);
        this.models = new API.Models(this);
        this.memoryBanks = new API.MemoryBanks(this);
        this.shields = new API.Shields(this);
        this._options = options;
    }
    defaultQuery() {
        return this._options.defaultQuery;
    }
    defaultHeaders(opts) {
        return {
            ...super.defaultHeaders(opts),
            ...this._options.defaultHeaders,
        };
    }
}
exports.LlamaStackClient = LlamaStackClient;
_a = LlamaStackClient;
LlamaStackClient.LlamaStackClient = _a;
LlamaStackClient.DEFAULT_TIMEOUT = 60000; // 1 minute
LlamaStackClient.LlamaStackClientError = Errors.LlamaStackClientError;
LlamaStackClient.APIError = Errors.APIError;
LlamaStackClient.APIConnectionError = Errors.APIConnectionError;
LlamaStackClient.APIConnectionTimeoutError = Errors.APIConnectionTimeoutError;
LlamaStackClient.APIUserAbortError = Errors.APIUserAbortError;
LlamaStackClient.NotFoundError = Errors.NotFoundError;
LlamaStackClient.ConflictError = Errors.ConflictError;
LlamaStackClient.RateLimitError = Errors.RateLimitError;
LlamaStackClient.BadRequestError = Errors.BadRequestError;
LlamaStackClient.AuthenticationError = Errors.AuthenticationError;
LlamaStackClient.InternalServerError = Errors.InternalServerError;
LlamaStackClient.PermissionDeniedError = Errors.PermissionDeniedError;
LlamaStackClient.UnprocessableEntityError = Errors.UnprocessableEntityError;
LlamaStackClient.toFile = Uploads.toFile;
LlamaStackClient.fileFromPath = Uploads.fileFromPath;
exports.LlamaStackClientError = Errors.LlamaStackClientError, exports.APIError = Errors.APIError, exports.APIConnectionError = Errors.APIConnectionError, exports.APIConnectionTimeoutError = Errors.APIConnectionTimeoutError, exports.APIUserAbortError = Errors.APIUserAbortError, exports.NotFoundError = Errors.NotFoundError, exports.ConflictError = Errors.ConflictError, exports.RateLimitError = Errors.RateLimitError, exports.BadRequestError = Errors.BadRequestError, exports.AuthenticationError = Errors.AuthenticationError, exports.InternalServerError = Errors.InternalServerError, exports.PermissionDeniedError = Errors.PermissionDeniedError, exports.UnprocessableEntityError = Errors.UnprocessableEntityError;
exports.toFile = Uploads.toFile;
exports.fileFromPath = Uploads.fileFromPath;
(function (LlamaStackClient) {
    LlamaStackClient.Telemetry = API.Telemetry;
    LlamaStackClient.Agents = API.Agents;
    LlamaStackClient.Datasets = API.Datasets;
    LlamaStackClient.Evaluate = API.Evaluate;
    LlamaStackClient.Evaluations = API.Evaluations;
    LlamaStackClient.Inference = API.Inference;
    LlamaStackClient.Safety = API.Safety;
    LlamaStackClient.Memory = API.Memory;
    LlamaStackClient.PostTraining = API.PostTraining;
    LlamaStackClient.RewardScoringResource = API.RewardScoringResource;
    LlamaStackClient.SyntheticDataGenerationResource = API.SyntheticDataGenerationResource;
    LlamaStackClient.BatchInference = API.BatchInference;
    LlamaStackClient.Models = API.Models;
    LlamaStackClient.MemoryBanks = API.MemoryBanks;
    LlamaStackClient.Shields = API.Shields;
})(LlamaStackClient = exports.LlamaStackClient || (exports.LlamaStackClient = {}));
exports = module.exports = LlamaStackClient;
exports.default = LlamaStackClient;
//# sourceMappingURL=index.js.map