import { APIResource } from "../resource.js";
import * as Core from "../core.js";
import * as BatchInferenceAPI from "./batch-inference.js";
import * as Shared from "./shared.js";
import * as AgentsAPI from "./agents/agents.js";
export declare class BatchInference extends APIResource {
    chatCompletion(params: BatchInferenceChatCompletionParams, options?: Core.RequestOptions): Core.APIPromise<BatchChatCompletion>;
    completion(params: BatchInferenceCompletionParams, options?: Core.RequestOptions): Core.APIPromise<Shared.BatchCompletion>;
}
export interface BatchChatCompletion {
    completion_message_batch: Array<Shared.CompletionMessage>;
}
export interface BatchInferenceChatCompletionParams {
    /**
     * Body param:
     */
    messages_batch: Array<Array<Shared.UserMessage | Shared.SystemMessage | Shared.ToolResponseMessage | Shared.CompletionMessage>>;
    /**
     * Body param:
     */
    model: string;
    /**
     * Body param:
     */
    logprobs?: BatchInferenceChatCompletionParams.Logprobs;
    /**
     * Body param:
     */
    sampling_params?: Shared.SamplingParams;
    /**
     * Body param:
     */
    tool_choice?: 'auto' | 'required';
    /**
     * Body param: `json` -- Refers to the json format for calling tools. The json
     * format takes the form like { "type": "function", "function" : { "name":
     * "function_name", "description": "function_description", "parameters": {...} } }
     *
     * `function_tag` -- This is an example of how you could define your own user
     * defined format for making tool calls. The function_tag format looks like this,
     * <function=function_name>(parameters)</function>
     *
     * The detailed prompts for each of these formats are added to llama cli
     */
    tool_prompt_format?: 'json' | 'function_tag';
    /**
     * Body param:
     */
    tools?: Array<BatchInferenceChatCompletionParams.Tool>;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace BatchInferenceChatCompletionParams {
    interface Logprobs {
        top_k?: number;
    }
    interface Tool {
        tool_name: 'brave_search' | 'wolfram_alpha' | 'photogen' | 'code_interpreter' | (string & {});
        description?: string;
        parameters?: Record<string, AgentsAPI.ToolParamDefinition>;
    }
}
export interface BatchInferenceCompletionParams {
    /**
     * Body param:
     */
    content_batch: Array<string | Array<string>>;
    /**
     * Body param:
     */
    model: string;
    /**
     * Body param:
     */
    logprobs?: BatchInferenceCompletionParams.Logprobs;
    /**
     * Body param:
     */
    sampling_params?: Shared.SamplingParams;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace BatchInferenceCompletionParams {
    interface Logprobs {
        top_k?: number;
    }
}
export declare namespace BatchInference {
    export import BatchChatCompletion = BatchInferenceAPI.BatchChatCompletion;
    export import BatchInferenceChatCompletionParams = BatchInferenceAPI.BatchInferenceChatCompletionParams;
    export import BatchInferenceCompletionParams = BatchInferenceAPI.BatchInferenceCompletionParams;
}
//# sourceMappingURL=batch-inference.d.ts.map