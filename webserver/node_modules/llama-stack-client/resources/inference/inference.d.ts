import { APIResource } from "../../resource.js";
import { APIPromise } from "../../core.js";
import * as Core from "../../core.js";
import * as InferenceAPI from "./inference.js";
import * as Shared from "../shared.js";
import * as AgentsAPI from "../agents/agents.js";
import * as EmbeddingsAPI from "./embeddings.js";
import { Stream } from "../../streaming.js";
export declare class Inference extends APIResource {
    embeddings: EmbeddingsAPI.Embeddings;
    chatCompletion(params: InferenceChatCompletionParamsNonStreaming, options?: Core.RequestOptions): APIPromise<InferenceChatCompletionResponse>;
    chatCompletion(params: InferenceChatCompletionParamsStreaming, options?: Core.RequestOptions): APIPromise<Stream<InferenceChatCompletionResponse>>;
    chatCompletion(params: InferenceChatCompletionParamsBase, options?: Core.RequestOptions): APIPromise<Stream<InferenceChatCompletionResponse> | InferenceChatCompletionResponse>;
    completion(params: InferenceCompletionParams, options?: Core.RequestOptions): Core.APIPromise<InferenceCompletionResponse>;
}
export interface ChatCompletionStreamChunk {
    event: ChatCompletionStreamChunk.Event;
}
export declare namespace ChatCompletionStreamChunk {
    interface Event {
        delta: string | Event.ToolCallDelta;
        event_type: 'start' | 'complete' | 'progress';
        logprobs?: Array<InferenceAPI.TokenLogProbs>;
        stop_reason?: 'end_of_turn' | 'end_of_message' | 'out_of_tokens';
    }
    namespace Event {
        interface ToolCallDelta {
            content: string | Shared.ToolCall;
            parse_status: 'started' | 'in_progress' | 'failure' | 'success';
        }
    }
}
export interface CompletionStreamChunk {
    delta: string;
    logprobs?: Array<TokenLogProbs>;
    stop_reason?: 'end_of_turn' | 'end_of_message' | 'out_of_tokens';
}
export interface TokenLogProbs {
    logprobs_by_token: Record<string, number>;
}
export type InferenceChatCompletionResponse = InferenceChatCompletionResponse.ChatCompletionResponse | ChatCompletionStreamChunk;
export declare namespace InferenceChatCompletionResponse {
    interface ChatCompletionResponse {
        completion_message: Shared.CompletionMessage;
        logprobs?: Array<InferenceAPI.TokenLogProbs>;
    }
}
export type InferenceCompletionResponse = InferenceCompletionResponse.CompletionResponse | CompletionStreamChunk;
export declare namespace InferenceCompletionResponse {
    interface CompletionResponse {
        completion_message: Shared.CompletionMessage;
        logprobs?: Array<InferenceAPI.TokenLogProbs>;
    }
}
export type InferenceChatCompletionParams = InferenceChatCompletionParamsNonStreaming | InferenceChatCompletionParamsStreaming;
export interface InferenceChatCompletionParamsBase {
    /**
     * Body param:
     */
    messages: Array<Shared.UserMessage | Shared.SystemMessage | Shared.ToolResponseMessage | Shared.CompletionMessage>;
    /**
     * Body param:
     */
    model: string;
    /**
     * Body param:
     */
    logprobs?: InferenceChatCompletionParams.Logprobs;
    /**
     * Body param:
     */
    sampling_params?: Shared.SamplingParams;
    /**
     * Body param:
     */
    stream?: boolean;
    /**
     * Body param:
     */
    tool_choice?: 'auto' | 'required';
    /**
     * Body param: `json` -- Refers to the json format for calling tools. The json
     * format takes the form like { "type": "function", "function" : { "name":
     * "function_name", "description": "function_description", "parameters": {...} } }
     *
     * `function_tag` -- This is an example of how you could define your own user
     * defined format for making tool calls. The function_tag format looks like this,
     * <function=function_name>(parameters)</function>
     *
     * The detailed prompts for each of these formats are added to llama cli
     */
    tool_prompt_format?: 'json' | 'function_tag';
    /**
     * Body param:
     */
    tools?: Array<InferenceChatCompletionParams.Tool>;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace InferenceChatCompletionParams {
    interface Logprobs {
        top_k?: number;
    }
    interface Tool {
        tool_name: 'brave_search' | 'wolfram_alpha' | 'photogen' | 'code_interpreter' | (string & {});
        description?: string;
        parameters?: Record<string, AgentsAPI.ToolParamDefinition>;
    }
    type InferenceChatCompletionParamsNonStreaming = InferenceAPI.InferenceChatCompletionParamsNonStreaming;
    type InferenceChatCompletionParamsStreaming = InferenceAPI.InferenceChatCompletionParamsStreaming;
}
export interface InferenceChatCompletionParamsNonStreaming extends InferenceChatCompletionParamsBase {
    /**
     * Body param:
     */
    stream?: false;
}
export interface InferenceChatCompletionParamsStreaming extends InferenceChatCompletionParamsBase {
    /**
     * Body param:
     */
    stream: true;
}
export interface InferenceCompletionParams {
    /**
     * Body param:
     */
    content: string | Array<string>;
    /**
     * Body param:
     */
    model: string;
    /**
     * Body param:
     */
    logprobs?: InferenceCompletionParams.Logprobs;
    /**
     * Body param:
     */
    sampling_params?: Shared.SamplingParams;
    /**
     * Body param:
     */
    stream?: boolean;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace InferenceCompletionParams {
    interface Logprobs {
        top_k?: number;
    }
}
export declare namespace Inference {
    export import ChatCompletionStreamChunk = InferenceAPI.ChatCompletionStreamChunk;
    export import CompletionStreamChunk = InferenceAPI.CompletionStreamChunk;
    export import TokenLogProbs = InferenceAPI.TokenLogProbs;
    export import InferenceChatCompletionResponse = InferenceAPI.InferenceChatCompletionResponse;
    export import InferenceCompletionResponse = InferenceAPI.InferenceCompletionResponse;
    export import InferenceChatCompletionParams = InferenceAPI.InferenceChatCompletionParams;
    export import InferenceChatCompletionParamsNonStreaming = InferenceAPI.InferenceChatCompletionParamsNonStreaming;
    export import InferenceChatCompletionParamsStreaming = InferenceAPI.InferenceChatCompletionParamsStreaming;
    export import InferenceCompletionParams = InferenceAPI.InferenceCompletionParams;
    export import Embeddings = EmbeddingsAPI.Embeddings;
    export import EmbeddingCreateParams = EmbeddingsAPI.EmbeddingCreateParams;
}
//# sourceMappingURL=inference.d.ts.map