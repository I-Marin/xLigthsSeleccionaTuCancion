import { APIResource } from "../../resource.js";
import * as Core from "../../core.js";
import * as PostTrainingAPI from "./post-training.js";
import * as DatasetsAPI from "../datasets.js";
import * as JobsAPI from "./jobs.js";
export declare class PostTraining extends APIResource {
    jobs: JobsAPI.Jobs;
    preferenceOptimize(params: PostTrainingPreferenceOptimizeParams, options?: Core.RequestOptions): Core.APIPromise<PostTrainingJob>;
    supervisedFineTune(params: PostTrainingSupervisedFineTuneParams, options?: Core.RequestOptions): Core.APIPromise<PostTrainingJob>;
}
export interface PostTrainingJob {
    job_uuid: string;
}
export interface PostTrainingPreferenceOptimizeParams {
    /**
     * Body param:
     */
    algorithm: 'dpo';
    /**
     * Body param:
     */
    algorithm_config: PostTrainingPreferenceOptimizeParams.AlgorithmConfig;
    /**
     * Body param:
     */
    dataset: DatasetsAPI.TrainEvalDataset;
    /**
     * Body param:
     */
    finetuned_model: string;
    /**
     * Body param:
     */
    hyperparam_search_config: Record<string, boolean | number | string | Array<unknown> | unknown | null>;
    /**
     * Body param:
     */
    job_uuid: string;
    /**
     * Body param:
     */
    logger_config: Record<string, boolean | number | string | Array<unknown> | unknown | null>;
    /**
     * Body param:
     */
    optimizer_config: PostTrainingPreferenceOptimizeParams.OptimizerConfig;
    /**
     * Body param:
     */
    training_config: PostTrainingPreferenceOptimizeParams.TrainingConfig;
    /**
     * Body param:
     */
    validation_dataset: DatasetsAPI.TrainEvalDataset;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace PostTrainingPreferenceOptimizeParams {
    interface AlgorithmConfig {
        epsilon: number;
        gamma: number;
        reward_clip: number;
        reward_scale: number;
    }
    interface OptimizerConfig {
        lr: number;
        lr_min: number;
        optimizer_type: 'adam' | 'adamw' | 'sgd';
        weight_decay: number;
    }
    interface TrainingConfig {
        batch_size: number;
        enable_activation_checkpointing: boolean;
        fsdp_cpu_offload: boolean;
        memory_efficient_fsdp_wrap: boolean;
        n_epochs: number;
        n_iters: number;
        shuffle: boolean;
    }
}
export interface PostTrainingSupervisedFineTuneParams {
    /**
     * Body param:
     */
    algorithm: 'full' | 'lora' | 'qlora' | 'dora';
    /**
     * Body param:
     */
    algorithm_config: PostTrainingSupervisedFineTuneParams.LoraFinetuningConfig | PostTrainingSupervisedFineTuneParams.QLoraFinetuningConfig | PostTrainingSupervisedFineTuneParams.DoraFinetuningConfig;
    /**
     * Body param:
     */
    dataset: DatasetsAPI.TrainEvalDataset;
    /**
     * Body param:
     */
    hyperparam_search_config: Record<string, boolean | number | string | Array<unknown> | unknown | null>;
    /**
     * Body param:
     */
    job_uuid: string;
    /**
     * Body param:
     */
    logger_config: Record<string, boolean | number | string | Array<unknown> | unknown | null>;
    /**
     * Body param:
     */
    model: string;
    /**
     * Body param:
     */
    optimizer_config: PostTrainingSupervisedFineTuneParams.OptimizerConfig;
    /**
     * Body param:
     */
    training_config: PostTrainingSupervisedFineTuneParams.TrainingConfig;
    /**
     * Body param:
     */
    validation_dataset: DatasetsAPI.TrainEvalDataset;
    /**
     * Header param: JSON-encoded provider data which will be made available to the
     * adapter servicing the API
     */
    'X-LlamaStack-ProviderData'?: string;
}
export declare namespace PostTrainingSupervisedFineTuneParams {
    interface LoraFinetuningConfig {
        alpha: number;
        apply_lora_to_mlp: boolean;
        apply_lora_to_output: boolean;
        lora_attn_modules: Array<string>;
        rank: number;
    }
    interface QLoraFinetuningConfig {
        alpha: number;
        apply_lora_to_mlp: boolean;
        apply_lora_to_output: boolean;
        lora_attn_modules: Array<string>;
        rank: number;
    }
    interface DoraFinetuningConfig {
        alpha: number;
        apply_lora_to_mlp: boolean;
        apply_lora_to_output: boolean;
        lora_attn_modules: Array<string>;
        rank: number;
    }
    interface OptimizerConfig {
        lr: number;
        lr_min: number;
        optimizer_type: 'adam' | 'adamw' | 'sgd';
        weight_decay: number;
    }
    interface TrainingConfig {
        batch_size: number;
        enable_activation_checkpointing: boolean;
        fsdp_cpu_offload: boolean;
        memory_efficient_fsdp_wrap: boolean;
        n_epochs: number;
        n_iters: number;
        shuffle: boolean;
    }
}
export declare namespace PostTraining {
    export import PostTrainingJob = PostTrainingAPI.PostTrainingJob;
    export import PostTrainingPreferenceOptimizeParams = PostTrainingAPI.PostTrainingPreferenceOptimizeParams;
    export import PostTrainingSupervisedFineTuneParams = PostTrainingAPI.PostTrainingSupervisedFineTuneParams;
    export import Jobs = JobsAPI.Jobs;
    export import PostTrainingJobArtifacts = JobsAPI.PostTrainingJobArtifacts;
    export import PostTrainingJobLogStream = JobsAPI.PostTrainingJobLogStream;
    export import PostTrainingJobStatus = JobsAPI.PostTrainingJobStatus;
    export import JobListParams = JobsAPI.JobListParams;
    export import JobArtifactsParams = JobsAPI.JobArtifactsParams;
    export import JobCancelParams = JobsAPI.JobCancelParams;
    export import JobLogsParams = JobsAPI.JobLogsParams;
    export import JobStatusParams = JobsAPI.JobStatusParams;
}
//# sourceMappingURL=post-training.d.ts.map